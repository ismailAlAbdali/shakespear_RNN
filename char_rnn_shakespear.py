# Name: Ismail Al Abdali  
# Project: Shakespear Char_CNN

# -*- coding: utf-8 -*-
"""char_rnn_shackspear.ipynb

Automatically generated by Colaboratory.

Original file is located at open it with Co lab notebook
    https://drive.google.com/file/d/1d8cjgVj7H297jaWzbrDM7kX26nuCofoy/view?usp=sharing

## imports
"""

import tensorflow as tf
from tensorflow import keras
import numpy as np
import time



# preproess the text into one-hot
def preprocess(text):
    tokenize =  np.array(tokenizer.texts_to_sequences(text)) - 1
    return tf.one_hot(tokenize,max_id)

# get next char 
def next_char(model,text, temperature = 1):
    x_new = preprocess([text]) 
    y_proba = model.predict(x_new)[0,-1:,:] # get the probiliaty distrubtion for last char
    rescaled_logits = tf.math.log(y_proba) / temperature # rescale the logithiem of predicted probs
    char_id = tf.random.categorical(rescaled_logits,num_samples=1) + 1 # generates a single integer that corresponds to the predicted next char in the sequence
    return tokenizer.sequences_to_texts(char_id.numpy())[0] # return the next predicted char

# get the text of a number of char passing to it a certain char limits and temperature to control how conservaite or exploratory the model should be.
def complete_text(model,text, n_chars = 50,temperature = 1):
    for _ in range(n_chars):
        text += next_char(model,text,temperature)
    return text # return the whole predict n_char sequence. 



### This code was use in Co Lab notebook. Please look at the link above to access the notebook
### Check if we have GPU

# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#   raise SystemError('GPU device not found')
# print('Found GPU at: {}'.format(device_name))

# ### mount google drive

# from google.colab import drive
# drive.mount('/content/drive')






## let's read the text file
filepath = "./tiny-shakespeare.txt"
with open(filepath) as file:
    shake_text = file.read()

#print(shake_text)

# lets encode every characeter in the text using keras tokeninzer

tokenizer = keras.preprocessing.text.Tokenizer(char_level = True) # char level true for Char-CNN
tokenizer.fit_on_texts([shake_text])

#tokenizer.get_config()

# """#### covert from seq to text and text to seq"""

# tokenizer.texts_to_sequences(["First"])

# tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])


# now that we have tokenized the text, let's encode the full text so that each character represented by it's id
[encoded] = np.array(tokenizer.texts_to_sequences([shake_text])) - 1 # substract one to get id's from 0-38 rather than 1-39

max_id = np.max(encoded) + 1

# in case we wanted to split our data.
dataset_size = 100
train_size = len(encoded) * dataset_size // 100

# we use 100% of the data for training. 
dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])

## now that we have got our train dataset, let's chop our data into windows

# lets used the data windows method to convert the the long sequence of chars into smaller once
n_steps = 100
window_length = n_steps + 1 # the windows length would be 101
dataset = dataset.window(window_length,shift = 1,drop_remainder=True) # creates nonoverlapping windows in our dataset such that
# they go from 0-100 .. 1-101 .. 2-102

# now let's convert the nested dataset into flat dataset
dataset = dataset.flat_map(lambda window: window.batch(window_length))

# make a random seed
np.random.seed(42)
tf.random.set_seed(42)

# lets use batches and batch our data
batch_size = 32
# shuffle the windows and put it in batches
dataset = dataset.shuffle(10000).batch(batch_size)
# put the sequence into input sequence , and target.  
dataset = dataset.map(lambda windows : (windows[:,:-1], windows[:,1:]))

# lets convert our data to one-hot
dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch,depth = max_id), Y_batch))
# add prefitching so that we can overlap the data to imporve training performace
dataset = dataset.prefetch(1)

# for e in dataset.take(3):
#     print(e)

# example shapes output for X_batch and Y_batch
for X_batch,Y_batch in dataset.take(1):
    print(X_batch.shape,Y_batch.shape)

"""## lets train our model"""


timestr = time.strftime("%Y%m%d-%H%M%S")

num_hidden_neruons = 128
n_epochs = 5
path = f"./project3_rnn_ep_{n_epochs}_{num_hidden_neruons}_2H_{timestr}"

# define the RNN model
def fit_shackspear_rnn(dataset,num_hidden_neruons = num_hidden_neruons,n_epochs = n_epochs,filepath = path):
  # use TF Sequential API and make two hidden layers for the RNN. for Output layer use softmax activation.
  model = keras.models.Sequential([
    keras.layers.GRU(num_hidden_neruons,return_sequences=True,input_shape = [None, max_id]),
    keras.layers.GRU(num_hidden_neruons,return_sequences = True),
    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation="softmax"))
  ])
  # compile the model with a loss sparse_categorical_crossentropy and adam optimizer

  model.compile(loss = "sparse_categorical_crossentropy", optimizer = "adam")

  # fit the the model
  model.fit(dataset,epochs = n_epochs)

  # save the model. 
  model.save(filepath)

  return model

Shakespear_RNN = fit_shackspear_rnn(dataset,n_epochs=5)


# save the model
Shakespear_RNN.save(path)



### Loss:
loss_history = Shakespear_RNN.history.history["loss"]
# print tht loss average
avg_loss = np.mean(loss_history)
print(f"Average Loss is {round(avg_loss,4)}")


## let's predict what our model learned.



texts_to_predict = ["How are yo", "Oman" , "sleep","Queen", "King"] # tokens to test our model with
outputs = [] # append the predictions 


### loading last trained model
n_chars = 100
for i in texts_to_predict:
  outputs.append(complete_text(Shakespear_RNN,i,n_chars = n_chars, temperature=1))

print(f"The RNN will predict the next sequence of {n_chars} chars")
print("_" * 50)
for i in range(len(outputs)):
  print(f"input \"{texts_to_predict[i]}\" predictions is")
  print(outputs[i])
  print("=" * 50)

