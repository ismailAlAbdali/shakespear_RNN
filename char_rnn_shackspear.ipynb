{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138508b3",
   "metadata": {
    "id": "e9f40699"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c53137",
   "metadata": {
    "executionInfo": {
     "elapsed": 4380,
     "status": "ok",
     "timestamp": 1679968651420,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "600f40e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970a173f",
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1679968732437,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "2719c114"
   },
   "outputs": [],
   "source": [
    "## let's read the text file\n",
    "filepath = \"/content/drive/MyDrive/Colab Notebooks/project3_rnn/tiny-shakespeare.txt\"\n",
    "with open(filepath) as file:\n",
    "    shake_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020dddab",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1679968733781,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "adc8f26b"
   },
   "outputs": [],
   "source": [
    "#print(shake_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7677b55d",
   "metadata": {
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1679969467968,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "b027082a"
   },
   "outputs": [],
   "source": [
    "# lets encode every characeter in the text using keras tokeninzer\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True) # char level true for Char-CNN\n",
    "tokenizer.fit_on_texts([shake_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc5f197e",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679969469102,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "6bea9fdb"
   },
   "outputs": [],
   "source": [
    "#tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6e1d5",
   "metadata": {
    "id": "0a40b946"
   },
   "source": [
    "#### covert from seq to text and text to seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66e4bde9",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679969469875,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "2631bd99"
   },
   "outputs": [],
   "source": [
    "#tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d340adec",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1679969470049,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "f9034416"
   },
   "outputs": [],
   "source": [
    "#tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d83d361",
   "metadata": {
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1679969471190,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "56a10154"
   },
   "outputs": [],
   "source": [
    "# now that we have tokenized the text, let's encode the full text so that each character represented by it's id\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shake_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d076c638",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1679969471362,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "629ea582",
    "outputId": "ec842c8f-f77b-49cd-fb94-cd272ea11faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19,  5,  8, ..., 20, 26, 10])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba1cb821",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1679969471364,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "7f4bbf28"
   },
   "outputs": [],
   "source": [
    "# for e in dataset.take(10):\n",
    "#     print(e)\n",
    "max_id = np.max(encoded) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d26a85a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679969472128,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "25286c1f",
    "outputId": "baef2ed1-0522-4a0d-e46d-712f8b8732e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 0, 1115394)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(encoded), np.min(encoded), len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ebd7e5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1679969474261,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "b1613204",
    "outputId": "1abb027f-76c0-4fe0-8365-3514f8351366"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13500"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15000 * 90 //100 # same as 15000 * (90 / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65d57c40",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1679969711290,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "zdvwXzkh_MAd"
   },
   "outputs": [],
   "source": [
    "dataset_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec1508a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1679969713052,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "49c8f75e"
   },
   "outputs": [],
   "source": [
    "train_size = len(encoded) * dataset_size // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f97f2d79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1679969714508,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "QtDrPzPJ-_4S",
    "outputId": "f274324b-07e0-4a06-88f1-c3347e0585d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c582a9e",
   "metadata": {
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1679969690183,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "bc593cb3"
   },
   "outputs": [],
   "source": [
    "dataset_size = tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2be78596",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1679969691242,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "7cab9700",
    "outputId": "cff3896f-1d95-441a-c8c6-f3244774fa64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40cf146c",
   "metadata": {
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1679969719019,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "cf277902"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac2b83",
   "metadata": {
    "id": "fb47e166"
   },
   "outputs": [],
   "source": [
    "## now that we have got our train dataset, let;s chop our data into windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82bd1d97",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1679969725687,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "63b0c876"
   },
   "outputs": [],
   "source": [
    "# lets used the data windows method to convert the the long sequence of chars into smaller once\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # the windows length would be 101\n",
    "dataset = dataset.window(window_length,shift = 1,drop_remainder=True) # creates nonoverlapping windows in our dataset such that\n",
    "# they go from 0-100 .. 1-101 .. 2-102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c068a404",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1679969728223,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "79bb04be",
    "outputId": "c998b623-b672-4ff3-a29b-7a4d4043900e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# now let's convert the nested dataset into flat dataset\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d71ca4",
   "metadata": {
    "id": "e06ed9e8",
    "outputId": "fe748729-7c40-423e-d513-6b335989366a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.data.ops.dataset_ops._NumpyIterator at 0x1df6fdd2380>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9db5d68",
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1679969745170,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "831804f4"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9431a6",
   "metadata": {
    "id": "db40c983"
   },
   "outputs": [],
   "source": [
    "#for e in dataset.take(100):\n",
    " #   print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925041c",
   "metadata": {
    "id": "8d96097c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d7feadf",
   "metadata": {
    "id": "908e7687"
   },
   "source": [
    "Machine leanring note: Gradient desent works best when when the instances of the training set are independent and identically distrubuted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9ed58a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1679969747449,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "eb7d1f62"
   },
   "outputs": [],
   "source": [
    "# lets use batches and batch our data\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows : (windows[:,:-1], windows[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced543e",
   "metadata": {
    "id": "8b4165a6"
   },
   "outputs": [],
   "source": [
    "# for e in dataset.take(10):\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79e773",
   "metadata": {
    "id": "fbb6ea13"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f358726a",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679969749999,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "26af167b"
   },
   "outputs": [],
   "source": [
    "# lets convert our data to one-hot\n",
    "dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch,depth = max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2da4b25",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1679969752073,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "3e68180a"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87863c50",
   "metadata": {
    "id": "a934e984"
   },
   "outputs": [],
   "source": [
    "# for e in dataset.take(3):\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d351b839",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2174,
     "status": "ok",
     "timestamp": 1679969755960,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "592a253b",
    "outputId": "cd36d899-ec0e-4233-9266-13e877d815aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch,Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape,Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3777ac3",
   "metadata": {
    "id": "6f6aaffd"
   },
   "source": [
    "## lets train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa1d101c",
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1679972861825,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "92ba8da4"
   },
   "outputs": [],
   "source": [
    "num_hidden_layers = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(num_hidden_layers,return_sequences=True,input_shape = [None, max_id]),\n",
    "    keras.layers.GRU(num_hidden_layers,return_sequences = True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abbeb9ae",
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1679972863835,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "cb2d4d62"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bcc96fbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1679972866060,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "027e51a3",
    "outputId": "9af01687-eead-4f83-c5d9-4acf1f6877ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_4 (GRU)                 (None, None, 128)         64896     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, None, 128)         99072     \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, None, 39)         5031      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,999\n",
      "Trainable params: 168,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57048e2",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c1377ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "34853/34853 [==============================] - 505s 14ms/step - loss: 0.9550\n",
      "Epoch 2/15\n",
      "34853/34853 [==============================] - 505s 14ms/step - loss: 0.9794\n",
      "Epoch 3/15\n",
      "34853/34853 [==============================] - 505s 14ms/step - loss: 1.0406\n",
      "Epoch 4/15\n",
      "34853/34853 [==============================] - 497s 14ms/step - loss: 1.0884\n",
      "Epoch 5/15\n",
      "34853/34853 [==============================] - 488s 14ms/step - loss: 1.1237\n",
      "Epoch 6/15\n",
      "34853/34853 [==============================] - 534s 15ms/step - loss: 1.1623\n",
      "Epoch 7/15\n",
      "34853/34853 [==============================] - 575s 16ms/step - loss: 1.1936\n",
      "Epoch 8/15\n",
      "34853/34853 [==============================] - 495s 14ms/step - loss: 1.2141\n",
      "Epoch 9/15\n",
      "34853/34853 [==============================] - 496s 14ms/step - loss: 1.2374\n",
      "Epoch 10/15\n",
      "34853/34853 [==============================] - 505s 14ms/step - loss: 1.2533\n",
      "Epoch 11/15\n",
      "34853/34853 [==============================] - 492s 14ms/step - loss: 1.2624\n",
      "Epoch 12/15\n",
      "34853/34853 [==============================] - 580s 17ms/step - loss: 1.2840\n",
      "Epoch 13/15\n",
      "34853/34853 [==============================] - 482s 14ms/step - loss: 1.2910\n",
      "Epoch 14/15\n",
      "34853/34853 [==============================] - 459s 13ms/step - loss: 1.2995\n",
      "Epoch 15/15\n",
      "34853/34853 [==============================] - 458s 13ms/step - loss: 1.3131\n"
     ]
    }
   ],
   "source": [
    "trained_RNN = model.fit(dataset,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4893c",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "iwHnSvDfLP26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_6_layer_call_fn, gru_cell_6_layer_call_and_return_conditional_losses, gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/content/drive/MyDrive/cs460g/project3/128_15_ep_char_rnn_shakespear_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff960963",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LpRKyiAXBVXa"
   },
   "outputs": [],
   "source": [
    "### Loss:\n",
    "loss_history = trained_RNN.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dad571",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W0VrZnCAJ_-F"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c6d4a3",
   "metadata": {
    "id": "014c03b7"
   },
   "source": [
    "### lets check if we can use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a604947",
   "metadata": {
    "id": "6566ce29"
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d93e39",
   "metadata": {
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1679968749910,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "f07b5512"
   },
   "outputs": [],
   "source": [
    "## let's try predicting\n",
    "\n",
    "def preprocess(text):\n",
    "    tokenize =  np.array(tokenizer.texts_to_sequences(text)) - 1\n",
    "    return tf.one_hot(tokenize,max_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69dd75be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679968751620,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "90e5fb94",
    "outputId": "c361f327-0caf-438a-e2e6-fbca6ada3d5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 39), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = preprocess([\"How are yo\"])\n",
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9066bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 184,
     "status": "error",
     "timestamp": 1679968753521,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "7888a3c4",
    "outputId": "4316dd70-c811-467e-a559-77e79821719b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d0f077bc4989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(x_new), axis = 1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e737383",
   "metadata": {
    "id": "2524de36",
    "outputId": "2a3dd6ca-142b-4f5e-ef4d-d052437e73ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7941b799",
   "metadata": {
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1679968761208,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "7da70efd"
   },
   "outputs": [],
   "source": [
    "def next_char(model,text, temperature = 1):\n",
    "    x_new = preprocess([text])\n",
    "    y_proba = model.predict(x_new)[0,-1:,:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits,num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "830062d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1679968762014,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "0ea6e512"
   },
   "outputs": [],
   "source": [
    "def complete_text(model,text, n_chars = 50,temperature = 1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model,text,temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0eaffe82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1679968764135,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "1a65f908",
    "outputId": "253a706c-6805-4c65-89c7-1c491824a3dd"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9e47fefde85e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplete_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Queen\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: complete_text() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"Queen\",n_chars=5,temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b35a89",
   "metadata": {
    "id": "375534b9",
    "outputId": "9becd5db-4c13-44fe-9736-8a301d736652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34857/34857 [==============================] - 142s 4ms/step\n",
      "34857/34857 [==============================] - 135s 4ms/step\n",
      "34857/34857 [==============================] - 140s 4ms/step\n",
      "34857/34857 [==============================] - 147s 4ms/step\n",
      "34857/34857 [==============================] - 145s 4ms/step\n",
      "wo a a\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"w\",n_chars=5,temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52023e4",
   "metadata": {
    "id": "fb62aba9",
    "outputId": "3b28109d-7c9e-4ea1-f76f-ea1d672606c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_12_layer_call_fn, gru_cell_12_layer_call_and_return_conditional_losses, gru_cell_13_layer_call_fn, gru_cell_13_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 128_2_hidden_GRU_Shackspear_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 128_2_hidden_GRU_Shackspear_model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"128_2_hidden_GRU_Shackspear_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de7d9b",
   "metadata": {
    "id": "00665fb4"
   },
   "source": [
    "### loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84fd1583",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "error",
     "timestamp": 1679954072091,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "86125f93",
    "outputId": "82cd4a69-f6b7-4215-db72-205fcc1fd636"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d07950c5933a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"128_2_hidden_GRU_Shackspear_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         raise IOError(\n\u001b[0m\u001b[1;32m    228\u001b[0m                             \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at 128_2_hidden_GRU_Shackspear_model"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6635d3a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21591,
     "status": "ok",
     "timestamp": 1679968697019,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "S9RCu6RtEQZ9",
    "outputId": "d8d0e48b-8344-4586-ecf9-4caa9a793fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f411f28",
   "metadata": {
    "executionInfo": {
     "elapsed": 9351,
     "status": "ok",
     "timestamp": 1679968706358,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "Oyvs-2XyD6dU"
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/project3_rnn/128_2_hidden_GRU_Shackspear_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fd94730",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1679968706361,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "5150c86f",
    "outputId": "62fe8310-f1cd-4195-a2e5-bc22d3d49261"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "### Check if we have GPU\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f653898e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17707,
     "status": "ok",
     "timestamp": 1679954193489,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "wNLOwvCdEly9",
    "outputId": "0d1ccdc1-fffc-4c7e-f567-7a9b6b7227e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
      "CPU (s):\n",
      "6.343770006\n",
      "GPU (s):\n",
      "0.09363167000000772\n",
      "GPU speedup over CPU: 67x\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d56f5e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12833,
     "status": "ok",
     "timestamp": 1679968786623,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "RGEZ0bP_EmRE",
    "outputId": "d7b02281-c20a-4263-9ca8-c62580a5b5b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 1s 699ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "text = complete_text(loaded_model,\"run\",n_chars = 100,temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b482ba8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1679968786624,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "OKT6k1b1FAhU",
    "outputId": "9d3dd278-3e96-4edd-d714-6745e8b49eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rund\n",
      "betution, the great at the seasel like a create\n",
      "to wome glesting here!\n",
      "\n",
      "caliban:\n",
      "foul this island;\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06cc34d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679969218594,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "ATzV7J8vG3ml",
    "outputId": "71284b96-2e99-44fb-9df2-a1839c72a736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 1, 'epochs': 1, 'steps': 1}\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model.history.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fde11917",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "error",
     "timestamp": 1679972485615,
     "user": {
      "displayName": "Ismail Alabdali",
      "userId": "00595816703033863117"
     },
     "user_tz": 240
    },
    "id": "9Wnb9Igo8g0g",
    "outputId": "2b6ba84b-6666-45f1-c004-7bf677aa6ff1"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-d8bd133e9f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "loaded_model.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d705101",
   "metadata": {
    "id": "gV-p5xurKcae"
   },
   "source": [
    "### running all the code together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f15baef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n",
      "Epoch 1/5\n",
      "     44/Unknown - 15s 169ms/step - loss: 3.1965"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 154\u001b[0m\n\u001b[0;32m    150\u001b[0m   model\u001b[38;5;241m.\u001b[39msave(filepath)\n\u001b[0;32m    152\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 154\u001b[0m Shakespear_RNN \u001b[38;5;241m=\u001b[39m \u001b[43mfit_shackspear_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n\u001b[0;32m    158\u001b[0m Shakespear_RNN\u001b[38;5;241m.\u001b[39msave(path)\n",
      "Cell \u001b[1;32mIn[2], line 147\u001b[0m, in \u001b[0;36mfit_shackspear_rnn\u001b[1;34m(dataset, num_hidden_neruons, n_epochs, filepath)\u001b[0m\n\u001b[0;32m    144\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# fit the the model\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# save the model. \u001b[39;00m\n\u001b[0;32m    150\u001b[0m model\u001b[38;5;241m.\u001b[39msave(filepath)\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Desktop\\Spring 2023\\CS460G\\project3\\project_3_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"char_rnn_shackspear.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at open it with Co lab notebook\n",
    "    https://drive.google.com/file/d/1d8cjgVj7H297jaWzbrDM7kX26nuCofoy/view?usp=sharing\n",
    "\n",
    "## imports\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# preproess the text into one-hot\n",
    "def preprocess(text):\n",
    "    tokenize =  np.array(tokenizer.texts_to_sequences(text)) - 1\n",
    "    return tf.one_hot(tokenize,max_id)\n",
    "\n",
    "# get next char \n",
    "def next_char(model,text, temperature = 1):\n",
    "    x_new = preprocess([text]) \n",
    "    y_proba = model.predict(x_new)[0,-1:,:] # get the probiliaty distrubtion for last char\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature # rescale the logithiem of predicted probs\n",
    "    char_id = tf.random.categorical(rescaled_logits,num_samples=1) + 1 # generates a single integer that corresponds to the predicted next char in the sequence\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0] # return the next predicted char\n",
    "\n",
    "# get the text of a number of char passing to it a certain char limits and temperature to control how conservaite or exploratory the model should be.\n",
    "def complete_text(model,text, n_chars = 50,temperature = 1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model,text,temperature)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "### This code was use in Co Lab notebook. Please look at the link above to access the notebook\n",
    "### Check if we have GPU\n",
    "\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "# ### mount google drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## let's read the text file\n",
    "filepath = \"./tiny-shakespeare.txt\"\n",
    "with open(filepath) as file:\n",
    "    shake_text = file.read()\n",
    "\n",
    "#print(shake_text)\n",
    "\n",
    "# lets encode every characeter in the text using keras tokeninzer\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level = True) # char level true for Char-CNN\n",
    "tokenizer.fit_on_texts([shake_text])\n",
    "\n",
    "#tokenizer.get_config()\n",
    "\n",
    "# \"\"\"#### covert from seq to text and text to seq\"\"\"\n",
    "\n",
    "# tokenizer.texts_to_sequences([\"First\"])\n",
    "\n",
    "# tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n",
    "\n",
    "\n",
    "# now that we have tokenized the text, let's encode the full text so that each character represented by it's id\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shake_text])) - 1 # substract one to get id's from 0-38 rather than 1-39\n",
    "\n",
    "max_id = np.max(encoded) + 1\n",
    "\n",
    "# in case we wanted to split our data.\n",
    "dataset_size = 100\n",
    "train_size = len(encoded) * dataset_size // 100\n",
    "\n",
    "# we use 100% of the data for training. \n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "## now that we have got our train dataset, let's chop our data into windows\n",
    "\n",
    "# lets used the data windows method to convert the the long sequence of chars into smaller once\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # the windows length would be 101\n",
    "dataset = dataset.window(window_length,shift = 1,drop_remainder=True) # creates nonoverlapping windows in our dataset such that\n",
    "# they go from 0-100 .. 1-101 .. 2-102\n",
    "\n",
    "# now let's convert the nested dataset into flat dataset\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# make a random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# lets use batches and batch our data\n",
    "batch_size = 32\n",
    "# shuffle the windows and put it in batches\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "# put the sequence into input sequence , and target.  \n",
    "dataset = dataset.map(lambda windows : (windows[:,:-1], windows[:,1:]))\n",
    "\n",
    "# lets convert our data to one-hot\n",
    "dataset = dataset.map(lambda X_batch, Y_batch : (tf.one_hot(X_batch,depth = max_id), Y_batch))\n",
    "# add prefitching so that we can overlap the data to imporve training performace\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# for e in dataset.take(3):\n",
    "#     print(e)\n",
    "\n",
    "# example shapes output for X_batch and Y_batch\n",
    "for X_batch,Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape,Y_batch.shape)\n",
    "\n",
    "\"\"\"## lets train our model\"\"\"\n",
    "\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "num_hidden_neruons = 128\n",
    "n_epochs = 5\n",
    "path = f\"./project3_rnn_ep_{n_epochs}_{num_hidden_neruons}_2H_{timestr}\"\n",
    "\n",
    "# define the RNN model\n",
    "def fit_shackspear_rnn(dataset,num_hidden_neruons = num_hidden_neruons,n_epochs = n_epochs,filepath = path):\n",
    "  # use TF Sequential API and make two hidden layers for the RNN. for Output layer use softmax activation.\n",
    "  model = keras.models.Sequential([\n",
    "    keras.layers.GRU(num_hidden_neruons,return_sequences=True,input_shape = [None, max_id]),\n",
    "    keras.layers.GRU(num_hidden_neruons,return_sequences = True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,activation=\"softmax\"))\n",
    "  ])\n",
    "  # compile the model with a loss sparse_categorical_crossentropy and adam optimizer\n",
    "\n",
    "  model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\")\n",
    "\n",
    "  # fit the the model\n",
    "  model.fit(dataset,epochs = n_epochs)\n",
    "\n",
    "  # save the model. \n",
    "  model.save(filepath)\n",
    "\n",
    "  return model\n",
    "\n",
    "Shakespear_RNN = fit_shackspear_rnn(dataset,n_epochs=5)\n",
    "\n",
    "\n",
    "# save the model\n",
    "Shakespear_RNN.save(path)\n",
    "\n",
    "\n",
    "\n",
    "### Loss:\n",
    "loss_history = Shakespear_RNN.history.history[\"loss\"]\n",
    "# print tht loss average\n",
    "avg_loss = np.mean(loss_history)\n",
    "print(f\"Average Loss is {round(avg_loss,4)}\")\n",
    "\n",
    "\n",
    "## let's predict what our model learned.\n",
    "\n",
    "\n",
    "texts_to_predict = [\"How are yo\", \"Oman\" , \"sleep\",\"Queen\", \"King\"] # tokens to test our model with\n",
    "outputs = [] # append the predictions \n",
    "\n",
    "\n",
    "### loading last trained model\n",
    "n_chars = 100\n",
    "for i in texts_to_predict:\n",
    "  outputs.append(complete_text(Shakespear_RNN,i,n_chars = n_chars, temperature=1))\n",
    "\n",
    "print(f\"The RNN will predict the next sequence of {n_chars} chars\")\n",
    "print(\"_\" * 50)\n",
    "for i in range(len(outputs)):\n",
    "  print(f\"input \\\"{texts_to_predict[i]}\\\" predictions is\")\n",
    "  print(outputs[i])\n",
    "  print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09606553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
